{"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"abIPUQ1LBeq_"},"source":["# Evaluación de los Sistemas de Recomendación\n","\n","Para poder determinar la bondad de los resultados proporcionados por un sistema de recomendación, existe un conjunto de medidas de calidad estandarizadas. Estas medidas tienen por objetivo medir la calidad de las recomendaciones proporcionadas por el sistema de recomendación y compararlo con otros sistemas de recomendación para determinar cuál es el que mejor se ajusta nuestras necesidades.\n","\n","Tradicionalmente, las medidas de calidad se empleaban para medir la bondad de las predicciones, las recomendaciones y las listas de recomendaciones. Sin embargo, en los últimos tiempos, se ha detectado una importante carencia de estas medidas para evaluar otros factores más subjetivos de las recomendaciones. Estas nuevas medidas se engloban dentro de las denominadas \"*beyond accuracy*\", cuya traducción más literal podría considerarse como \"*más allá de la precisión*\". \n","\n","Dentro de estas medidas podemos encontrar:\n","\n","- ***Novelty***. Permite evaluar la novedad de las recomendaciones. Por ejemplo, un sistema de recomendación que indique que nos va a gustar *El Padrino* si no la hemos valorado se considera poco novedoso, sin embargo, si nos indica que nos va a gustar una película de cine alternativo que poca gente conoce se considera muy novedoso.\n","\n","- ***Stability***. Esta medida busca medir la estabilidad del sistema de recomendación frente a la nueva información incorporada al mismo. Lo deseable es que un sistema que ya disponga de cientos de miles de votos no varíe demasiado las recomendaciones proporcioandas cuando se incorporen unos pocos votos nuevos.\n","\n","- ***Diversity***. Esta medida se centra en evaluar la diversidad en las recomendaciones. Por ejemplo, si estamos buscando un restaurante para ir a cenar, no queremos que el sistema de recomendación nos proporciones únicamente restaurantes italianos, es preferible que nos indique cierta variedad de comida para que podamos elegir la que más nos interese.\n","\n","Con el fin de replicar de la forma más certera posible un escenario de recomendación real, es necesario dividir el *dataset* empleado para las recomendaciones en dos: **entrenamiento** y **test**. La parte de **entrenamiento** permitirá \"ajustar\" el sistema para que pueda proporcionar recomendaciones. La parte de **test** permitirá comparar las votaciones reales de los usuarios con las recomendaciones proporcionadas por el sistema para medir la calidad de las mismas.\n","\n","En la mayoría de los problemas de aprendizaje automático, esta división es trivial. Sin embargo, cuando tratamos con sistemas de recomendación, es necesario hacer una división específica para este tipo de problemas. En primer lugar, debemos dividir los usuarios en conjuntos de entrenamiento y test. Los **usuarios de entrenamiento** serán usuarios del sistema que se emplean únicamente para entrenar el sistema. Por su parte, los **usuarios de test** serán usuarios sobre los que se calcularán las recomendaciones que serán contrastadas con las medidas de calidad. Sin embargo, si simplemente dividiéramos a los usuarios, las predicciones realizadas deberían hacerse sobre los votos de los usuarios de test que ya han intervenido en el proceso de entramiento, falseando de este modo las mediciones del sistema. Por lo tanto, además de dividir los usuarios, debemos dividir los items en items entrenamiento y test. Los **items de entrenamiento** serán los items votados por los usuarios de test con los que se inferirán sus preferencias. Los **items de test** serán los items votados por los usuarios de test sobre los que aplicaremos la evaluación mediante medias de calidad. La siguiente figura muestra conceptualmente esta división:\n","\n","<img src=\"https://github.com/KNODIS-Research-Group/ml-notebooks/blob/main/sistemas%20de%20recomendacion/figures/evaluacion.png?raw=true\" alt=\"Particionado del dataset para evaluacion\">\n","\n","En las siguientes secciones se definirán conceptual y formalmente las diferentes medidas de calidad existentes. Para ello emplearemos las siguientes definiciones:\n","\n","- Sea $U$ el conjunto de **usuarios de entrenamiento**.\n","- Sea $U^T$ el conjunto de **usuarios de test**.\n","- Sea $I$ el conjunto de **items de entrenamiento**.\n","- Sea $I^T$ el conjunto de **items de test**.\n","- Sea $r_{u,i}$ la **votación** del usuario $u$ al item $i$.\n","- Sea $\\hat{r}_{u,i}$ la **predicción** del voto del usuario $u$ al item $i$.\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"4Ah_0p2AUxcC"},"source":["## Evaluación de las predicciones\n","\n","Las medidas de calidad de las predicciones tienen como objetivo evaluar lo certeras que han sido las predicciones realizadas por el sistema de recomendación. Estas medidas comparan el valor de la predicciones del voto ($\\hat{r}_{u,i}$) con la votación de test real emitida los el usuario ($r_{u,i}$). Disponemos de diferentes alternativas para su implementación.\n","\n","Definimos el **error medio absoluto** (*Mean Absolute Error | MAE*) como la diferencia media en valor absoluto de los votos y las predicciones. Definimos el *MAE* del usuario $u$ como:\n","\n","$$MAE_u = \\frac{ \\sum_{i \\in I^T_u} \\mid r_{u,i} - \\hat{r}_{u,i} \\mid  }{\\#I^T_u} $$\n","\n","donde $I^T_u$ representa el conjunto de items de test votados por el usuario $u$.\n","\n","Definimos el *MAE* del sistema como el promedio del *MAE* de cada usuario:\n","\n","$$MAE = \\frac{ \\sum_{u \\in U^T} MAE_u }{ \\#U^T } $$\n","\n","Definimos el **error cuadrático medio** (*Mean Squared Error | MSE*) como la diferencia cuadrática media de los votos y las predicciones. Este error penaliza más los errores grandes que el *MAE*. Definimos el *MSE* del usuario $u$ como:\n","\n","$$MSE_u = \\frac{ \\sum_{i \\in I^T_u} ( r_{u,i} - \\hat{r}_{u,i} )^2  }{\\#I^T_u} $$\n","\n","Definimos el *MSE* del sistema como el promedio del *MSE* de cada usuario:\n","\n","$$MSE = \\frac{ \\sum_{u \\in U^T} MSE_u }{ \\#U^T } $$\n","\n","Definimos la **raíz del error cuadrático medio** (*Root Mean Squared Error | RMSE*) como la raíz de la diferencia cuadrática media de los votos y las predicciones. Definimos el *RMSE* del usuario $u$ como:\n","\n","$$RMSE_u = \\sqrt{ \\frac{ \\sum_{i \\in I^T_u} ( r_{u,i} - \\hat{r}_{u,i} )^2  }{\\#I^T_u} }$$\n","\n","Definimos el *RMSE* del sistema como el promedio del *RMSE* de cada usuario:\n","\n","$$RMSE = \\frac{ \\sum_{u \\in U^T} RMSE }{ \\#U^T } $$"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ma6OU41nU13h"},"source":["## Evaluación de las recomendaciones\n","\n","Las medidas de calidad de las recomendaciones tienen como objetivo evaluar lo satisfactorias que han resultados las recomendaciones proporcionadas a un usuario. Incluir el concepto de satisfacción dentro de las medidas de calidad requiere una binarización de los posibles votos que puede emitir un usuario en dos categorías: me gusta y no me gusta. Generalmente, esta binarización se produce mediante un umbral ($\\theta$) que indica a partir de qué valor se considera relevante (me gusta) un voto. Por ejemplo, si las votaciones plausibles van de 1 a 5 estrellas podemos establecer el umbral igual a 4 ($\\theta = 4$) y, de este modo, se consideran como relevantes los votos 4 y 5, y como no relevantes los votos 1, 2 y 3.\n","\n","Basándonos en este concepto, definimos la **precisión** como la proporción de los items recomendados a un usuario que le han resultado relevantes (le han gustado). Definimos *precision* del usuario *u* como:\n","\n","$$precision_u = \\frac {\\# \\{i \\in R_u | r_{u,i} \\geq \\theta\\}} {N}$$\n","\n","Donde $R_u$ representa los items de test recomendados al usuarios $u$ y $N$ es el número de items recomendados. Es habitual denominar a esta medida como $precision_u@N$. Por tanto, definiremos *precision* como el promedio de la precisión de cada usuario de test:\n","\n","$$precision = \\frac{ \\sum_{u \\in U^T} precision_u }{ \\#U^T } $$\n","\n","Definimos el **recall** como la proporción de los items recomendados a un usuario que le han resultados relevantes respecto del total de items relevantes de dicho usuario. Definimos el *recall** del usuario *u* como:\n","\n","$$recall_u = \\frac {\\# \\{i \\in R_u | r_{u,i} \\geq \\theta\\}} {\\# \\{i \\in I^T_u | r_{u,i} \\geq \\theta  \\}}$$\n","\n","Donde $R_u$ representa los items de test recomendados al usuario $u$. Es habitual denominar a esta medida como $recall_u@N$. Por tanto, definiremos *recall* como el promedio del recall de cada usuario de test:\n","\n","$$recall = \\frac{ \\sum_{u \\in U^T} recall_u }{ \\#U^T } $$\n","\n","Generalmente, todo sistema de recomendación, trata de maximizar su precisión para lograr que a un usuario le resulten interesantes la mayoría de items que se le recomienden. No obstante, es posible que, a pesar de tener una precisión baja, se estén obteniendo uno buenos resultados de recomendación siempre y cuando el recall sea cercano a 1. \n","\n","Para encontrar un equilibrio entre precisión y recall se suele emplear **F1**. Definimos el $F1$ del usuario $u$ como:\n","\n","$$F1_u = 2 \\cdot \\frac{ precision_u \\cdot recall_u }{ precision_u + recall_u }$$\n","\n","Asimismo, definiremos *F1* como el promedio de la F1 de cada usuario de test:\n","\n","$$F1 = \\frac{ \\sum_{u \\in U^T} F1_u }{ \\#U^T }$$\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"rwu0599i0E4s"},"source":["## Evaluación de las listas de recomendaciones\n","\n","Las medidas de evaluación de las recomendaciones suelen reportar unos resultados bastante representativos del funcionamiento del sistema. Sin embargo, sus resultados no siempre se ajustan correctamente a la satisfacción de los usuarios con las recomendaciones recibidas. Por lo general, un sistema de recomendación proporciona a cada usuario una lista ordenada que contiene *N* recomendaciones. La primera de recomendación de dicha lista será el item que el sistema considera que se ajusta más al perfil del usuario. La segunda, el segundo item que mejor lo haga y así sucesivamente.\n","\n","Las medidas de precisión, recall y F1 no tienen en consideración en qué posición de la lista estaba el item en el que acertamos o fallamos una predicción. Sin embargo, desde el punto de vista del usuario, el sentido común indica que son asumibles errores en las últimas posiciones de la lista de recomendaciones, pero no en las primeras. Ilustremos esto con un ejemplo. Supongamos dos sistemas de recomendación que proporciona 10 recomendaciones a los usuarios. Para un usuario aleatorio, el primera sistema acierta en las 5 primeras recomendaciones y falla en las 5 últimas. Para ese mismo usuario, el segundo sistema falla las 5 primeras recomendaciones y acierta las 5 últimas. Ambos sistemas tienen una precisión de 0.5, pero ¿qué sistema ofrece más confianza al usuario?\n","\n","**nDCG (*normalized Discounted Cumulative Gain*)** es una medida de calidad basada en listas de recomendaciones. Básicamente se encarga de comparar las recomendaciones proporcionadas en una lista con las recomendaciones que idealmente deberían haberse producido. Esta medida además da mucha más importancia a los aciertos (o los fallos) producidos en las primeras posiciones de la lista que a los que se producen en las últimas.\n","\n","Definimos el *nDCG* del usuario *u* como el *Discounted Cumulative Gain* dividido entre el *Ideal Discounted Cumulative Gain*:\n","\n","$$nDCG_u = \\frac {DCG_u} {IDCG_u}$$\n","\n","$$DCG_u = \\sum_{i \\in R_u} \\frac {2^{r_{u,i}} - 1} {log_2(pos(i)+1)}$$\n","\n","$$IDCG_u = \\sum_{i \\in I^T_u} \\frac {2^{r_{u,i}} - 1} {log_2(ipos(i)+1)}$$\n","\n","Donde $R_u$ representa el conjunto de items de test recomendados al usuario $u$, $I^T_u$ representa los items de test votados por el usuario $u$, $pos(i)$ representa la posición que ocupa el item $i$ en la lista de recomendaciones y $ipos(i)$ representa la posición que ocupa el item $i$ en la lista idea de recomendaciones (esto es, los items de test del usuario ordenados de mayor a menor valoración).\n","\n","Finalmente, definimos el $nDCG$ del sistema como el promedio de dicha medida para todos los usuarios de test:\n","\n","$$nDCG = \\frac{ \\sum_{u \\in U^T} nDCG_u }{ \\#U^T }$$\n","\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"bZFCgYfttoIs"},"source":["---\n","\n","*Este documento ha sido desarrollado por **Fernando Ortega**. Dpto. Sistemas Informáticos, ETSI de Sistemas Informáticos, Universidad Politécnica de Madrid.*\n","\n","*Última actualización: Marzo de 2024*\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"XNzuk4vz-1zr"},"source":["<p xmlns:cc=\"http://creativecommons.org/ns#\" >This work is licensed under <a href=\"http://creativecommons.org/licenses/by-nc/4.0/?ref=chooser-v1\" target=\"_blank\" rel=\"license noopener noreferrer\" style=\"display:inline-block;\">CC BY-NC 4.0<img style=\"height:22px!important;margin-left:3px;vertical-align:text-bottom;\" src=\"https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1\"><img style=\"height:22px!important;margin-left:3px;vertical-align:text-bottom;\" src=\"https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1\"><img style=\"height:22px!important;margin-left:3px;vertical-align:text-bottom;\" src=\"https://mirrors.creativecommons.org/presskit/icons/nc.svg?ref=chooser-v1\"></a></p>"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"2. Evaluación de los Sistemas de Recomendación.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":0}
